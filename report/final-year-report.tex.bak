\documentclass[a4wide,leqno,12pt]{report}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx} %%graphics in pdfLaTeX
\usepackage[export]{adjustbox}
\usepackage{times}
\parindent 0in
\parskip 0.05in
%\usepackage{setspace}
%\doublespacing
\linespread{1.5}
\usepackage{svg}
\usepackage{float}

\begin{document}

\bgroup %% <<<<<<< begin a group
\linespread{0}
\thispagestyle{empty}
\begin{center}
{\sffamily
{\Large University of Dublin}

\vspace{10pt}

\includegraphics[scale=0.12]{images/trinitycollege.pdf}
%\includegraphics[scale=.5]{t.pdf}

\vspace{10pt}

{\Huge TRINITY COLLEGE}

\vspace{80pt}

\textbf{ \Large \emph A Survey of Web Servers in\\ Trinity College Dublin}

\vspace{30pt}

Michael Power

B.A. (Mod.) Integrated Computer Science

Final Year Project May 2018

Supervisor: Dr. Stephen Farrell

\vspace{110pt}

\large{School of Computer Science and Statistics
\\$ $\\
O'Reilly Institute, Trinity College, Dublin 2, Ireland}
\linespread{1}
}
\end{center}
\egroup %% <<<<<<< end a group

\chapter*{Declaration}
I hereby declare that this thesis is entirely my own work and that it
has not been submitted as an exercise for a degree at any other
university.

\begin{center}
\vspace*{2in}

\underline{\hspace*{3in}} \today

Michael Power
\end{center}

\chapter*{Permission to Lend}

I agree that the Library and other agents of
the College may lend or copy this thesis upon request.

\begin{center}
\vspace*{2in}

\underline{\hspace*{3in}} \today

Michael Power


\end{center}


\chapter*{Acknowledgements}
To be done

\newpage



\pagestyle{headings}
\tableofcontents
\listoffigures


\begin{abstract}
\noindent
As the number of Internet devices grows, so to does the difficultly to monitor these devices effectively. This report details the use of ZMap a port scanner and ZGrab an application layer scanner within Trinity College Dublin To survey Web Servers. System administrators have often hundreds of hosts to consider when monitoring Web Servers. The use of the above tools to audit these Web Servers in order to deal with security issues that if left unattended could potential lead to further problems is of the utmost importance for any organization that aims to mitigate such risks, as well as using these tools to study vulnerabilities in order to better defend from attacks, since the availability of tools such as these leads to the potential of attackers finding vulnerability hosts. Scanning at an Internet wide level has shown great promise for uncovering security problems as well as showing the state of public facing web servers  \cite{durumeric2015search} thus the same should be true at a University Campus level.\\

As well as deploying and testing the tool within Trinity College Dublin, this report also hope to be able to interpret the output, and communicate that to site owners/system admins in order to help make their web a bit better and more secure.

\end{abstract}


\chapter{Introduction}
\section{Goals of Report}
!!!!!!!!!!!!!!!MIND TENSE USED (PAST OR FUTURE), PICK ONE AND KEEP IT, ALSO MIND THE USE OF I WHEN DESCRIBING THE REPORT!!!!!!!!!!!!!!!\\



As with any device that is connected to the Internet, the  security capabilities of these systems is of concern. Even when following strict policies,  miss configurations of Web Servers can lead to possible issues down the line. The aim of this project is to deploy a local instance of ZMap and ZGrab within Trinity College Dublin, running scans on both port 80 and 443 in order to survey web servers, in an effort to build a picture of what the current state of Web Servers within the college campus looks like.\\

This report  will also outline the steps that were taken in order for other organizations and institutions to replicate what has been done here within Trinity College Dublin.\\


This goal will be analyzed with the following questions in mind:
\begin{itemize}
  \item Which IP addresses are listening on port 80, port 443 and both ports?
  \item The variation in the number of host on at a certain time?
  \item How many IP addresses have a Hostname?
  \item Trying to categorize IP addresses?
  \item What does the current state of Certificates look like within the college?
  \item How many Certificates are self signed certs?
  \item How many Certificates are browser trusted?
   \item How many Certificates have expired at the time of writing this report?
  \item Are there any out of date versions of TLS being used?
  \item For those IP addresses that don't have a Hostname, do their certificates lead to one?
  \item What Signature and Key Algorithms are being used?
  \item Is secure renegotiation false for any IP addresses?
  \item Are there any public keys that are used across multiple IP addresses?
  \item The variation in public key lengths?
  \item How many IP addresses are managing redirects correctly?
 
\end{itemize}
\section{Outline}
The remainder of this report is organized as follows:\\
\begin{itemize}
\item\textbf{Chapter 2} aims to familiarise the reader with the required background information.
\item\textbf{Chapter 3} highlights other relevant work conducted in the area of network scanning.
\item\textbf{Chapter 4} explains the overall design of the investigation as well as the ethical concerns it entails.
\item\textbf{Chapter 5} details the implementation of ZMap and ZGrab within Trinity College Dublin as well as describing the various programs and scripts used to gather the data.
\item\textbf{Chapter 6} presents the results of the study conducted.
\item\textbf{Chapter 7} provides an analysis of the findings in surveying Web Servers within Trinity College Dublin.
\item\textbf{Chapter 8} suggest future work that could be done following the findings of the work done in this project.
\end{itemize}


\chapter{Background}
This chapter will introduce Web Servers along with there underlying technology. It will also discuss the main scanning tools used to conduct the study.
\section{Web Server}

Web servers are one of the many components that make up the overall Web Application Architecture.
Communication between between a Web Server and a Clients browser is typical done through a protocol called Hyper Text Transfer Protocol or HTTP for short. The general way in which a Client initiates communication between a web server is through a Web Browser such as Chrome or Firefox. The browser sends a request to the web sever of some resource typically a web page or some other file type, if the resource doesn't exist an error is returned to the client, generally in the form of a 404 or some other status code. As well as handling requests, the server may sometimes be required to handle processing of forms inputted by the client\cite{conallen1999modeling}. Almost any device that is connected to the Internet can host a web server, such as a printer or mobile phone. \\

Web Servers play a significant role in terms of the overall workings of web applications and security issues relating to web servers can come in a number of scenarios such as being incorrectly configured\cite{mendes2008assessing} or lacking proper privilege management.
\section{Port}
Port numbers are the original and most extensively used means for application and service identification on the Internet. Ports are 16-bit numbers, and the combination of source and destination port umbers together with the IP addresses of the communicating end systems uniquely identifies a session of a given transport protocol \cite{cotton2011internet} such as TCP(Transmission control protocol) which provides reliability and safety of packets sent  or UDP (User Datagram Protocol) unlike TCP does not guarantee delivery of packets, UDP is widely used in streaming of videos where speed is more important than reliability \cite{khan2016transport}. Some Port numbers are also known by their associated service names or protocol used for the communication such as "https" for port number 443 and "http" for port number 80. Port numbers range from 0 to 65535 with Port 0 being reserved, leaving 65535 that could be used. There are three distinct classes of Port numbers:

\begin{itemize}
\item the System Ports, also known as the Well Known Ports, from 0-1023
      (assigned by IANA)
\item the User Ports, also known as the Registered Ports, from 1024-
      49151 (assigned by IANA)
\item the Dynamic Ports, also known as the Private or Ephemeral Ports,
      from 49152-65535 (never assigned)
\end{itemize}
\section{HTTP and HTTPS}
The Hypertext transfer Protocol (HTTP) is application layer protocol most commonly associated with the world wide web ("www"). HTTP connections are made on Port 80, HTTP does not encrypt the data that is exchanged between the client and server \cite{fielding1999hypertext}. This is solved with HTTPS or HTTP over TLS which provides a secure session between the client and server encrypting the data exchange between the two parties \cite{rescorla2000http}. 

\section{Transport Layer Security}
TLS or SSL as it may sometimes be referred to is a protocol that allows secure communication between a client and server over the Internet. Over the years there have been many versions of TLS/SSL implementations, the first being in 1994 when Netscape developed SSLv1.O however this was never released publicly as it was vulnerable to replay attacks\cite{turner2014transport}. This drove the development of SSLv2.0 in 1995, but along with its predecessor it too had problems. In total there were 4 main deficiencies with SSLv2.0, one of these for example was not protecting handshake messages which grants a man-in-the-middle to trick the client in selecting a weaker cipher suite than it would normally choose \cite{turner2011prohibiting}. Then in 1996 SSLv3.0 was distributed to combat the issues with SSLv2.0 gaining a large popularity in the process \cite{turner2014transport}. In mid 1996 development of these protocols moved to the Internet Engineering Task Force (IETF) and so too did the name of the protocol changing from SSL to TLS. To date there have been four releases of TLS with TLSv1.0 being released in 1999, TLSv1.1 in 2006, TLSv1.2 in 2008\cite{turner2014transport} and in 2014 a draft of TLSv1.3 was developed
 \cite{dierks2014transport}.\\
 \begin{center}
\includegraphics[scale=.5]{images/tls_demo.png}
\end{center}
In order for a secure connection to be established the client first sends a \textit{Client Hello} message which includes the version of TLS supported by the client a random byte string that is used for generating the pre-master secret,  along with the list of cipher suites and compression methods in order of the clients preference \cite{tlsDemoServer}.A session identifier is also included to identify the session between the client and server\cite{turner2014transport}.
The server responds with a \textit{ServerHello} message indicating the version of TLS that will be used along with the appropriate cipher suite, compression method and session identifier. The server also generates its own random byte string. Next a \textit{Certificate} is sent from the server to the Client whenever the key exchange method uses certificates. This certificate message contains a chain of certificates which the client will use to validate the server says who they say they are. Following this a Server Key Exchange Message is sent if the certificate sent by the server lacks sufficient data for the client to exchange a pre-master secret.The Server then sends a \textit{ServerHelloDone} message to the Client to signal that the client can now continue with its turn of the key exchange related messages. The Client now sends a \textit{ClientKeyExchange} message to the server which includes the pre-master secret created by the client. Both the Client and the Server send a \textit{ChangeCipherSpec} message to alert each other that all future exchanges within this session will be encrypted with the chosen cipher suite and keys. Finally the Client sends a \textit{Finished} Message to the Server and likewise The Server sends a \textit{Finished } message to the Client, both of theses exchanges are used in order to verify that the key exchange and authentication process were successful as well as indicating that each respective part of the TLS handshake is complete. From this point on all \textit{ApplicationData} exchange between the client and server is protected based on the current connection state \cite{tlsDemoServer}\cite{turner2014transport}.\\

\section{Port Scanning}

Port scanners are tools that are used to check for devices on open ports, these tools could be used by system administrator to monitor devices on a network or by a malicious party hoping to find vulnerable devices to infect.\\
There are three main grades of port scans that can be conducted:
\begin{enumerate}
\item\textbf{Vertical Scans:}
The vertical port scan is one in which a single host is scanned across multiple ports.
\item\textbf{Horizontal Scans:}
The horizontal scan is one in which a single port is scanned against multiple hosts.
\item\textbf{Block Scans:}
The Block scan is a combination of vertical and horizontal scans.\cite{lee2003detection}
\end{enumerate}

There are a number Port Scanning tools that are widely available such as Nmap \cite{lyon2009nmap} and Nessus \cite{beale2004nessus}, this project exclusively uses ZMap \cite{durumeric2013zmap} to conduct the Port Scans.
\subsection{ZMap}

!!!!!!!!!!!!!! SHOW AND DESCRIBE SAMEPLE OUTPUT !!!!!!!!!!!!!\\


ZMap is an open-source network scanner optimized for efficiently performing
Internet-scale network surveys \cite{durumeric2013zmap}, developed in 2013 by Zakir Durumeric, Eric Wustrow and J. Alex Halderman at the University of Michigan they have been able to effectively diminish the time required to scan the entire IPv4 address space to a matter of minutes. The architecture allows sending and receiving components
to run asynchronously and enables a single source machine to comprehensively scan every host in the public IPv4
address space for a particular open TCP port in under 45 minutes using a 1 Gbps Ethernet link\cite{durumeric2013zmap}. The default configuration for ZMap unfortunately only allows scans for one specified port in the given IP range (CIDR Block)\cite{fuller1993classless}, which in this study and many others requires users of this tool to write further programmes to investigate hosts on more than one specified port. ZMap currently has fully implemented probe modules for TCP SYN scans, ICMP, DNS queries, UPnP, BACNET, and can send a large number of UDP probes \cite{zmapGithub}\\


In order to randomise the scans conducted on the target address space, ZMap remains stateless. To avoid keeping state ZMap sends a fixed number of probs per target with the default sending only one probe.\cite{durumeric2013zmap}\\

One of the problems of Internet wide scanning is that it uses a large amount of bandwidth,
the creators of ZMap have introduced seven points in relation to the best practices when conducting these scans\cite{durumeric2013zmap} page 12. \\


\begin{center}
\centering
\includegraphics[scale=.4]{images/zmap_architecture.png}
\end{center}

Figure 1. ZMap Architecture.\\


ZMap uses a modular design to support many types of probes and integration with a variety of research applications. Instructions are entered Via the Command Line (CLI) which are then parsed to the \textit{State \& Config} module. The next two modules \textit{Validation Generation} and
\textit{Address Generation} are responsible for generating the target space/address range while also validating this space to ensure any host/IP addresses in this generation are excluded according to a configuration file which omits sites/IP ranges such as any reserved address allocations along with individual host who may wish to opt out of future scans \cite{durumeric2013zmap} while the \textit{Probe Scheduler} sets the timing of the soon to be sent probes. Next we have the \textit{Packet Generation} module which is responsible for generating the required probe packets as per the type of scan we are conducting, the \textit{Packet Transmission} module is responsible for sending these packets to their intended destination. ZMap also provides Extensible probe modules which can be customized for different kinds of probes. The \textit{Framework Monitoring} oversees every packet that is sent and received, while the \textit{Receipt \& Validation} module responds to TCP SYN-ACK packets and discard packets clearly not initiated by the scan by cross checking the source and destination ports of the packets. The \textit{Response Interpretation} interprets the responses from those that have been validated by the \textit{Receipt \& Validation} module. Before the results of the probes are outputted they are parsed by the \textit{Result Processing} module which processes the results and outputs them either to the console via stdout or to a comma separated file (csv), the results can also be piped to another process directly such as ZGrab \cite{durumeric2013zmap}.\\


\begin{figure}[H]
\centering
\includegraphics[scale=.3]{pdf_images/zmap_scan_exmaple_2}
\caption{a nice plot1}
\label{fig:zmap_scan_example}
\end{figure}


The speed at which ZMap sends packets is performed as fast as the source's CPU or NIC allows. This speed however at which ZMap sends probes is a cause for concern, as sending them in numerical order would probably overload and cause a network failure. So in order to counteract this ZMap uses a random permutation of the address space, iterating over a multiplicative group of integers modulo p, with p being slightly larger than $2^{32}$. By choosing p to be a prime, ZMap guarantees that the group is cyclic and
will reach all addresses in the IPv4 address space once per cycle. To select a new permutation for each scan, a new primitive root of the multiplicative group and a new random starting address are chosen. ZMap efficiently finds random primitive roots of
the  multiplicative  group  by  utilizing  the  isomorphism $(Zp - 1,+)~=(Z*p,x)$ and  mapping  roots of$(Zp-1,+)$ into the multiplicative group via the  function f(x) =nx where n is a known primitive root of$(Z/pZ)x$. Once this primitive root is ZMap cycles through the target address space by applying the group operation to the current address. The scan is finished once the initially scanned IP address is reached\cite{durumeric2013zmap}.\\

ZMap send packets at Ethernet Level in order to cache packet values and reduce the overhead on the Kernel.
ZMap implements a probing technique known as
SYN scanning or half-open scanning \cite{durumeric2013zmap}. This was chosen instead of performing a full TCP handshake
based on the reduced number of exchanged packets. In
the situation where a host is unreachable or does
not respond, only a single packet is used in the exchange (a SYN from
the scanner); in the case of a closed port, two packets
are exchanged (a SYN answered with a RST); and in the
situation where the port is open, three packets are
exchanged (a SYN, a SYN-ACK reply, and a RST from
the scanner which will close the connection)\cite{durumeric2013zmap}.


\section{Banner Grabbing}
Banner grabbing is a technique used to gain information about a device on a network, this is obtained by establishing a connection with the device and observing the output from the connection \cite{kondo2014penetration}. System Administrators can use these tools designed for this application to take inventory of the devices and services on their network. Attackers can also use banner grabbing tools in order to find network devices that are using known applications with well documented vulnerability (MD5 for example).


\subsection{ZGrab}
!!!!!!!!!!!!!! SHOW AND DESCRIBE SAMEPLE OUTPUT !!!!!!!!!!!!!\\

ZGrab is one such banner grabber implemented in Go \cite{pike2009go} that allows user to perform various application handshakes for a number of different protocols such as HTTP, HTTPS, SSH \cite{ylonen2006secure} as well as SMTP\cite{2012smtp} to name a few. ZGrab connects to the host by opening up a TCP connections \cite{durumeric2015search}. ZGab outputs the information in raw JSON format retrieving all the information about the connection handshake such as SSL/TLS information as well as response codes.

For example When performing a TLS handshake with a host, ZGrab offers the cipher suites implemented by the Golang TLS library and logs the chosen cipher suite\cite{durumeric2015search} rather than using the chosen cipher suite of the source machine performing the ZGrab. ZGrab can be also be used in conjunction with ZMap to grab service information simultaneous, while a host is being scanned or independently of ZMap by passing the host source IP address or domain name directly into ZGrab. Instructions are passed to ZGrab much in the same way of ZMap via a command line interface (CLI).\\

In terms of the actual banner grab there are a number of aspects of a web server that we are interested in when we conduct are ZGrab application layer scans in order to see the contrast and similarities underlying the service information of the web Servers in the college.\\
Here is a list of the fields that this project extracted to examine the web servers in the college:

\begin{itemize}
  \item Ciphersuite being used to provide encryption of data between the client and server.
  \item Key Exchange Method used to exchanged cryptographic keys between the client and server.
  \item TLS/SSL verison being used to provide secure communication between the client and server.
  \item The Public Key of the Web server embedded in the Certificate.
  \item The length of the Public Key used.
  \item Certificate start and end date, to see when the certificate was issued and also when it expires.
  \item the Signature Algorithm used to sign the Certificate.
  \item If the Certificate is self signed or not.
  A self signed certificate is a Certificate that has not been Issued and signed by a Certificate Authority rather the issuer who has created the certificate has signed it themselves with their own private key \cite{housley1998internet}.
  \item Browser Trusted field to see if the certificate is verified by the browser. This is done by the clients browser who attempts to build a chain of trust from the certificate to a root certificate on the client. The root trust store on the client contains a set of root certificates from trusted CAs to validate against. The browser also checks that the certificate has the correct hostname and the certificate has not yet expired \cite{acer2017wild}. If at any point there is a failure in the validation process, this means that the client(browser) is unsure of who the proper identity of the server actually is. 
  \item The Status code and Reason Phrase, The status code itself is a three digit integer that is received by the client as a response from the server, the phrase is a short description for the user \cite{fielding1999hypertext} \cite{berners1996hypertext}.
  \item HTML body of the web server's root page to categorise the servers and their uses.

\end{itemize}


\chapter{Related Work}
This chapter examines the current research around network wide scanning and it's uses, in particular the use of ZMap and ZGrab to conduct these scans. While the majority of literature found is intended at an Internet wide level, there are few studies found in this report that look at a more local view of a network in particular those host that are on port 80 and port 443.

Other similar scanning works on an Internet wide scale can be seen with Zakir Durumeric, Michael Bailey and J. Alex Halderman the creators of ZMap/ZGrab at the University of Michigan,  where in 2014 they published a paper\cite{durumeric2014internet} investigating the overall environment of scanners on the Internet by analysing a years worth of data from January 1st 2013 up until May 1st 2014 from a large network telescope revolving around scanning activity, with a scan being defined as an "instance where a source contacted at least 100 unique addresses in our darknet(.0018\% of the public IPv4 address space) on the same port and protocol at the minimum estimated Internet-wide scan rate of 10 packets per second (pps)", they also investigated the way in which organisations protected themselves against these scans if at all. Motivations behind many of scans that they discovered were in relation to academic research, while a large proportion of scans were targeting services with know vulnerabilities (e.g. SQL serves). Throughout there period of study 10.8 million scans from 1.76 million hosts were detected with the distribution of the scans found consisted of 56.4\% TCP SYN packets, 35.0\% UDP packets, and 8.6\% ICMP echo request packets. The HTTP and HTTPS were among the highest in terms of number of scans found on these protocols, which in relation to this project are the protocols most familiar with port 80 and 443. With malicious users and attackers having the ability to use these tools for various rouge purposes scan detection plays a huge part in defending organisations however according to there findings the vast majority don't regard scanning as a significant threat, that being said within 24 hours of new vulnerabilities being released on devices, they discovered an increase in the number of scans conducted on the ports commonly associated with those devices, for example regarding the disclosure of the Heart-bleed Bug \cite{durumeric2014matter} which was discovered in March 2014 and publicly disclosed on April 7th 2014. The vulnerability itself allows attackers to remotely dump arbitrary private data from many common and popular servers that support TLS. In the week following the public disclosure 53 scans from 27 host targeting HTTPS were observed, prior to the disclosure of the vulnerability 29 scans from 16 hosts were observed targeting HTTPS, highlighting the use of these scanning tools to identify possible weaknesses. \\

The University of Michigan performs regular scans for HTTPS hosts\cite{durumeric2014internet} in order to track the certificate authority ecosystems in an effort to analyse TLS certificates and the Certificate Authorities that sign them. Between the periods of April 2012 and June 2013 they managed to collect 33.6 million unique X.509 Certificates of which 6.2 million were browser trusted as well as identifying the most common CAs by leaf certificates issued, with GoDaddy.com, Inc accounting for 31\%. They also saw an increase of 23\% in the Alexa Top 1 million websites \cite{top20161} transitioning from HTTP to HTTPS. Keys and Signatures were also tracked to highlight how ZMap could be used to mitigate risk and act as defensive tool for researchers but this also has the flip side effect of an attacker using the tool to locate hosts suffering from a new vulnerability within minutes\cite{durumeric2013zmap}. With the research still finding some Certificate Authorities still using MD5 to sign their certificates \cite{durumeric2013analysis}. Within the scope of the data set within this project, ZGrab will be leveraged to obtain certificates found within the University, retrieving certain parameters such as the signature algorithm used to sign the certificate along with the number of self signed and browser trusted certificates within the University.\\


Censys begun in 2015 and is a platform created by the same team that designed ZMap that helps information security practitioners answer questions in an effort to discover new threats and assess there impact they may have. They regularly probe every public IP address and popular domain names through horizontal scans of the IPv4 address space, curating data over time to see changes in protocol adaption and make it accessible through an interactive search engine and API that allows users to pose questions such as "what percentage of HTTPS servers support SSLv3.0", by eliminating the labour intensive process of analysing gigabytes of data as well as lowering the entry barrier for researchers who might not have the performance capabilities to perform these scans\cite{durumeric2015search}. They have also played a central role in the discovery or analysis of some of the most significant Internet-scale vulnerabilities: FREAK, Logjam, DROWN, Heart bleed, and the Mirai botnet. Today however Censys has moved out of the University of Michigan and into it's own company in order to better serve and expand their capabilities , offering more enhanced services, technical support, and an even more complete and powerful view of the Internet \cite{censysWeb}.
Censys could also be used to identify public facing devices that may have been intended to be private within a network but unintentional found it's way to public Internet or be used to calculate the risk that known public facing devices could have on an organisation \cite{durumeric2015search} by using their website to investigate such questions.\\


In conjunction with researchers at the University of Michigan, researchers at The International Computer Science Institute and the University of Illinois Urban-Champaign in 2016 have conducted similar HTTPS surveys but by analysing certificates from a large body of sources instead of just one with an aim to obtain a better perspective of the HTTPS ecosystem. In total they combined 8 different data sets observing nearly 17 million unique browser trusted certificates which were valid during August 29 to September 8, 2016 which was the investigation period of their study. Of the 8 datasets analysed Censys(38\%) and CT logs(90.5\%) accounted for 99.4\% coverage of all certificates observed. They are currently working with both parties in order to reduce the discrepancy between either source in order to make each one more or less a near comprehensive view of trusted all public HTTPS certificates\cite{vandersloot2016towards}.\\

Researchers at Ajou University, have  implemented ZMap within their own University campus in aid of identifying hosts and comparing various scanning techniques such as FIN \cite{arkin1999network} and Xmas Scans \cite{arkin1999network}for identifying hosts by conducting scans on a number of ports including port 25 (smtp) , 80(http) and 443 (https). These optional scanning techniques are implemented by creating specific probe modules for ZMap \cite{lee2016implementation}. Some of the hosts they discovered included old web servers of a printers which allowed them to instruct the printer to print test pages as well as gaining access to password protected content of the printer with default passwords of the known devices still in use and commonly found on the Internet. While this report on the other hand uses exclusive SYN scanning \cite{de1999review} the default for ZMap with an aim to also identifying regular and irregular IP addresses running on port 80 and 443 as well analysing the current configurations of these Web Servers within the university. This report also hopes to discover similar devices on these ports that were found in Ajou University al as well as categorising those IP addresses found.\\

This report differs from previous studies in terms of the scope of our dataset as well as the direct line of the questions we intend to answer in order to highlight the state of Web Servers within Trinity College Dublin. \\


\chapter{Design}
This Chapter will outline the design process of the scans  along with the ethical implications and precautions taken when conducting the various scans of Trinity College Dublin on port 80 and port 443.
\section{Overview}
\begin{figure}[H]
\centering
\includegraphics[scale=.5,trim={0 6cm 0 0},clip]{pdf_images/design}
\caption{a nice plot}
\label{fig:design}
\end{figure}

As we can see from figure \ref{fig:design} in section 1 highlighted in blue,
the design of the scans consisted of two horizontal set of scans on port 80 and port 443 that would run every hour being executed by a scheduler. After this, the analysis of the raw data outputted by ZMap would take place next within section 2 in green. Once discovered which IP addresses were listening on which Port along with what IP addresses had an associated host name by doing a reverse DNS lookup in order for the ZGrab scans with the domain lookup option enabled to be conducted. Once the data was sufficiently analysed for the purpose of ZGrab scans, 4 sets of ZGrab scans were carried out, these consisted of a ZGrab domain lookup on port 80, a ZGrab domain lookup on port 443, IP ZGrab scan on port 80 as well as port 443 as seen in section 3 of figure \ref{fig:design}. Lastly the output of the ZGrab scans would be analysed looking for certain fields that would be of interest as outlined in Chapter 2.

\section{Ethics}
As with any type of scanning work that is being conducted, there are a number of ethical considerations to be aware, including how the scans should be conducted and who to inform of there existence. Prior to scanning we informed the system administrators within the college, stating the intent of the scans that were being conducted. We also decided to limit the number of ports that would be scanned to just port 80 and port 443 as a measure of precaution that the scans could potentially have the strain on the network. While there are no codes of conduct as of yet in relation to network scanning, the creators of ZMap offer a few useful best practice guidelines when performing such scans\cite{durumeric2015search} \cite{durumeric2013zmap}.\\

Another consideration to account for is the releasing of data, since the majority of IP addresses are not publicly visible and to keep anonymity no individual IP addresses or domain names are released in this report.
\chapter{Implementation}
This chapter will cover the implementation of the scans conducted within Trinity College Dublin. It will also give useful information to help others understand the challenges that occurred throughout this project, so others carrying out future scanning projects can avoid. All the code used can be found at the github account attached \cite{michaelGithub}.
\section{Technologies Used}
The majority of this project was implemented in Python and Shell scripts, here are a list of the main technologies used:
\begin{itemize}
  \item ZMap as discussed earlier is a port scanner that finds IP addresses listening on a port by searching either a single IP or range of IP addresses against the desired port.
  \item ZGrab is used to perform the banner grabs of the IP addresses found.
  \item Crontab was used to automate/schedule both the ZMap and ZGrab scans.
  \item The majority of Python code was used to analyse and sort the data, as well leverage some libraries with python such as the socket library \cite{socket} to perform both DNS and reverse DNS lookup.
  \item Shell Scripts programmes were used to package ZMap and ZGrab in order for it to run with crontab.
  \item Matplotlib \cite{matplotlib} is a python 2D plotting library used to generate the graphs in this report.
\end{itemize}

\section{ZMap Implementation}
In order to implement the ZMap scans as discussed in the design in Chapter 4, I packaged ZMap within a Shell Script that would be set to run every hour on the hour by crontab which is a time based Scheduler in linux. The ouput of the two ZMap scans would be appended to a csv file lying in the server.\\

To differentiate which IP addresses were listening on to one or more ports I created a python programme to find what IP addresses were listening on Port 80 and which IP addresses were listening on Port 443 doing this I was able to get the intersection of the two sets to determine the number of IP addresses that were listening on both Ports. The way in which I found out which IP addresses were actually listening to only port 80, port 443 and which were listening to both ports I used a key value dictionary to determine this with the key being the IP address and value being a list of Ports that particular IP address was one for example an IP address that was listening to only port 80 would have a value of [‘80’] likewise for an IP address that was only listening to port 443 would have a value of [‘443’] and for an IP address that was on Both ports would have a value of [‘80’,’443’] .

In order to determine what IP address resolved to a hostname so as too do a ZGrab with a domain lookup to determine if there where any different Certificate Alt names between the IP lookup result and its corresponding Domain Name lookup result. To implement this the socket library within python provided the capabilities too implement a reverse DNS lookup to obtain the hostnames.\\


\section{ZGrab Implementation}
To Implement the ZGrab scans fully took a bit more care as different types of hosts would have different fields present in some and not in others, this was discovered by testing a collection of hosts which were controlled on both sides in order to get a varied sample of preliminary test results that would be used to implement a python programme to extract the fields that were of interest within this project.\\
These hosts were as follows:

\begin{itemize}
  \item A host that was signed by a Certificate Authority.
  \item A host that had a self signed certificate.
  \item A 2006 printer on Port 80.
  \item A host that wasn't being maintained.
  \item As well as a random host on Port 80 and 443.
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[scale=.3,trim={0 17cm 0 0},clip]{pdf_images/error_zgrab}
\caption{a nice plot}
\label{fig:zgrab_error}
\end{figure}

These banner grabs that were conducted helped in order to see the difference of the JSON outputs that ZGrab gives. Similarly the ZGrab scans were implemented much in the same for the ZMap scans, except the ZGrab scans were also doing scans on hostnames as well as IP addresses on port 80 and port 443. To extract the fields that were of interest a pyhton programme was implemented that would take the JSON output of the ZGrab and collect those fields we wanted. In order to take account for the fields that could be either present or missing in any particular ZGrab output, exception handling was conducted on all fields being extracted, this was also necessary due to the fact that if certain IP wasn't on at the time a ZGrab scan was being conducted an error would be outputted in the JSON file along with either the IP or hostname present which is dependent on the lookup method used for ZGrab scan that was just executed, with the vast majority of fields of interest being absent as seen in figure \ref{fig:zgrab_error}.
\section{Challenges}

One of the Challenges encountered within this product concerned the scheduling of the cron jobs for the ZGrab scans. Throughout the ZGrab scans conducted not all of the original IP addresses that were found in the ZMap scan presented themselves in the ZGrab data this can be contributed to a number of different reasons, One of these is the fact that the college has a number of IP address that are up less than 90\% of the time period for which ZMap scans were conducted as well as ZMap being run every hour on the hour, where as in the original scheduling of ZGrab they were set up to run for every 2 hours on the hour and as a result the hours on either side might have have presented themselves with IP addresses that are only in those slots. The reason the cron job was scheduled to run every 2 hours was due to ZGrab taking some time to complete a banner grab especially when an IP or domain wasn't up causing ZGrab to wait for a default of 10 seconds before attempting to abandon the connection. Releasing this the default timeout of 10 seconds was reduced to 5 seconds as  well as running multiple ZGrab scans over smaller lists of IP addresses or domains every hour similar to the ZMap scan scheduling rather than a single ZGrab scan being conducted on the entire list of IP addresses on port 443 but rather splitting that list of IP addresses on port 443 into 2/3 smaller list with a dedicated ZGrab Script being executed by the crontab. These methods solved the problem of the lengthly time a ZGrab scan took took allowing us to implement more frequent ZGrab scans on smaller lists.

\chapter{Results}

For Comparison of Some results e.g average host per hour look at page 5 and 6 of \cite{durumeric2013zmap}\\

Other comparison \cite{durumeric2015search} page 9 percentage of cipher suites being used could have a new column with TCD figure 5\\

Another comparison can be found with \cite{lee2016implementation} on page 683 finding out of date printers that haven't been updated in a long time\\

Another comparison can be found with trying to see how many scans are necessary for discovering all the host \cite{durumeric2013analysis} page 4\\

Another result comparison could be trying to identify CA that sign certs who have had security issues in the past \cite{durumeric2013analysis} page 6\\

Another result comparison could be looking at key lengths \cite{durumeric2013analysis} page 7 and 8, also look for a paper with recommendations of cryptographic algorithms and key lengths\\

Another result comparison could be looking at signature algorithms, algorithms used to sign the certificates \cite{durumeric2013analysis} page 9\\

Another result comparison looking at ips/hosts sharing multiple public keys, certs e.g Stephens Research and \cite{durumeric2013analysis} page 10\\

Another result comparison could be seeing sites vulnerable to FREAK Attack \cite{vandersloot2016toward} page 54
vandersloot2016toward7\\


In this section we will
present the results of the scans and use them to answer the questions outlined in section 1, in order to better understand the state of Web Servers in the college.


\section{ZMap Results}
!!!!!!!!!!!!!!!!!FIND PERIODS WHEN SERVER WAS DOWN !!!!!!!!!!!!!!!!!!!!!!!!!


\subsection{Variation In IP addresses}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/VariationInIpAddressesOnPort80OverTime}
\caption{a nice plot}
\label{fig:port80ZMap}
\end{figure}

The ZMap scans took place between the 5th of February at 8pm up until and including the 26th of March at 6pm. However as we can see in figure \ref{fig:port80ZMap} during the periods
02-03-2018  up until 16-03-2018 no scans were detected as this was during the snow period which caused a power cut in the college, downing  Stephen's Server that was conducting the ZMap Scans on both Port 80 and 443. In absence of this there was a total of 827 observational periods in which the ZMap scans took place. We can also see in figure \ref{fig:port80ZMap} an exceptional high count of 1116 IP addresses was recorded on the 13th of February at 1am (2018-02-13T01), this high count was due to an unknown failure of the ZMap scan on port 443, as within this observational slice there were no subsequent IP addresses on port 443 to compare against.
\begin{table}
\centering
\begin{tabular}{||c c c c ||}
 \hline
  & Port 80 (only) & Port 443 (only) & Both Ports \\ [0.5ex]
 \hline\hline
 Monday & 367 & 383 & 377  \\
 Tuesday & 378 & 383 & 377 \\
 Wednesday & 377 & 383 & 377 \\
 Thursday & 372 & 383 & 377  \\
 Friday & 383 & 383 & 377 \\
 Saturday & 352 & 383 & 377\\
 Sunday & 349 & 383 & 377\\[1ex]
 \hline
\end{tabular}
\caption{Average Number of IP addresses across week}
\label{table:1}
\end{table}

Another intriguing  point to note is that the count of IP addresses throughout the week is steady, with the number of IP addresses on port 80 (only) rising throughout the weekdays and falling at the weekend.\\

\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/VariationInIpAddressesOnPort443OverTime}
\caption{a nice plot1}
\label{fig:port443ZMap}
\end{figure}



From figure \ref{fig:port443ZMap} we can see the failed scan that appeared on the 13th of Feburary at 1am which lead to the overly high count of IP addresses on port 80 (only) for this observational slice.

Another interesting incident that was observed is the drop in the average number of IP addresses from 145 before the server was cut off to to 105 after the server was back up and running.

\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/VariationInIpAddressesOnBothPortsOverTime}
\caption{a nice plot1}
\label{fig:portsBothZMap}
\end{figure}
From figure \ref{fig:portsBothZMap} we can see a similar trend that was observed in the figure \ref{fig:port80ZMap} with a higher number of IP addresses being present throughout the weekday and falling at the weekend.\\

There is also no dramatic change in the average number of IP addresses being present before and after the server cut off that was seen in figure \ref{fig:port443ZMap}

\subsection{Port Distribution}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/IPaddressestoPorts}
\caption{a nice plot1}
\label{fig:ports}
\end{figure}
In all there was a total of 1714 unique IP addresses that were seen throughout the ZMap scans, from figure \ref{fig:ports} we can see a breakdown of the 3 main categories of IP addresses Port 80 (only) which are IP addresses that are were only observed on Port 80 and no other port, Port 443 which were IP addresses only observed on Port 443 and no other port, and finally Both Ports which consisted of IP addresses that were on both Port 80 and 443 at some point throughout the ZMap scans.\\

We recorded 511 IP addresses on port 80 alone, followed by 146 IP addresses on port 443 and lastly 1057 IP addresses on Both ports.

\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/RegularVsIrregularIPaddresses}
\caption{a nice plot1}
\label{fig:portsIrreg}
\end{figure}

From the scans (figure \ref{fig:average_day}) we noticed a number of irregularities in terms of the number of IP addresses that would be up at any given hour, to investigate this aspect of the data we decided to filter the IP addresses in terms of those that are regular which in our case is
IP addresses that are seen 90\% of the time which is 744 from 827 observational slots. Everything else is consider an Irregular IP address.

The above figure \ref{fig:portsIrreg} shows the comparison between each port category, with a total of 1190 IP addresses being Regular with 336 being on Port 80 (only), 136 on Port 443 (only) and 718 being on Both Ports. Similarly we observed that 524 of the total IP addresses observed in the scans are Irregular, with 175 of those being on Port 80 (only), 10 being on Port 443 (only) and 339 on Both Ports.


\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/AverageDayinTrinityCollegeDublin}
\caption{a nice plot1}
\label{fig:average_day}
\end{figure}
When looking at all the IP addresses on average over a 24 hour period as we can see in figure \ref{fig:average_day} there is a large count of IP addresses between 11am to 4pm

Peak time on Average in the college is 12pm midday with 1311 IP addresses being present
Off-Peak time are 1 am and 5 am with both counting 1221 IP addresses being present.

\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/MaximumNumberOfIrregularIPaddressesInAnAverageDay}
\caption{a nice plot1}
\label{fig:irreg_ips}
\end{figure}
When analysing the hour that an Irregular IP address has ever been on such as in figure \ref{fig:irreg_ips} we see that at some point over the time of the scans particularly at 3pm there has been 470 of the 524 Irregular IP addresses active at this hour throughout the ZMap scans. On the opposite scale 259 IP addresses have be present at 5am at some point in time over the series of scans conducted. This might lead us to believe that the majority of Irregular IP address occurs at 5pm which is not the case when we look at the individual occurrences in figure \ref{fig:irreg_occurence}.

\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/TotalNumberOccurencesOfIrregularIPaddressesOnAverage}
\caption{a nice plot1}
\label{fig:irreg_occurence}
\end{figure}
When counting each occurence of an IP address on either port we see in figure \ref{fig:irreg_occurence} that there is a large discrepancy between the number of occurrence of an IP and the hour that each IP appears. For example we see in figure \ref{fig:irreg_occurence} that the number of occurrences that happen at 1am is 16014 where as at 3pm, 8777 occurrences have taken place throughout the ZMap scanning period which is the second largest number of occurrences. This large difference on further inspection of the data is due to 27 Unique IP address of which only 10 were on port 80 (only), 1 on port 443 (only) and 16 on both ports.



\begin{table}
\centering
\begin{tabular}{|| c c ||}
 \hline
  Total Occurrences & Occurrences at 1am  \\ [0.5ex]
 \hline\hline
 1520 & 720  \\
 1496 & 720 \\
 1354 & 716 \\
 1294 & 710  \\
 1444 & 710 \\
 1159 & 702 \\
 1018 & 698 \\
 729 & 688  \\
 1633 & 390 \\
 1446 & 388 \\[1ex]
 \hline
\end{tabular}
\caption{Top 10 Irregular IP addresses seen at 1am }
\label{table:2}
\end{table}

From figure \ref{table:2} we can see that the vast majority of occurrences at 1am account for a major period of the overall number of occurrences. of these 27 that cause this large difference, 17 of them have 50\% or greater with the number of occurrences that are seen at 1am compared to every other day in the week combined.
 

!!!!!!!!!!!!!!!!!!!!!!RE DO IPS TO HOSTNAME PIECHART !!!!!!!!!!!!!!!!!!!!
\subsection{Domain Names}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/IpstoHostname}

\end{figure}

In all there 669 IP addresses that have hostname, this makes up 39\% of the overall IP addresses found in the college of those 669 that did have have associating hostnames, 212 were found on port 80 (only), 34 on port 443 (only) and 423 on both ports.


\section{ZGrab Results}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/ZMapVsZGrab}
\caption{a nice plot1}
\label{fig:zmap_vs_zgrab}
\end{figure}

!!!!!!!!!!!!!!!!!!!!! MIGHT HAVE TO REDO GRAPH ABOVE !!!!!!!!!!!!!!\\

!!!!!!!!!!!!!!!!!!!! Also state how many Irregular IP addresses we gathered !!!!!!!!!!!!!!!!!!!!!!


The ZGrab scans were first conducted on the 2018-03-27 at 18:00 and finishing on 2018-04-23 at 15:00. In total we managed to collect service information of 1497 unique IP addresses. With 442 being only on port 80, 130 being on Port 443 only and 925 being open on both ports. The IP addresses that were open on both ports in the ZMap scans were not always found in our ZGrab scans. Of the 925 IP addresses that are open on both ports 669 of those we managed to collect service information being on port 443, 924 of the 925 IP addresses we managed to collect service information regarding port 80. These results seen in figure \ref{fig:zmap_vs_zgrab} show that not all the IP addresses original scanned in the ZMap scans were present in the ZGrab scans, the main reasons for this were outlined in the Challenges subsection within the Implementation Chapter  
\subsection{Response Codes}

!!!!!!!!!!!!!!!!!!!!!!!!!!!DO RESPONSE CODE TABLE !!!!!!!!!!!!!!!!!!!!!!!!!!!



\subsection{TLS/SSL}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/TLSVersionsInTCD}
\caption{a nice plot1}
\label{fig:TLS_SSL}
\end{figure}

Over the course of the banner grabs we have found 3 different versions of TLS/SSL being implemented in the college to provide encryption of data. It turns out that SSL 3.0 is still being used despite the fact that it is inherently broken and suffers from various attacks such as POODLE\cite{holz2015summarizing} \cite{moller2014poodle} and many organisations \cite{ssllabs} advise not to use it  but use TLSv1.2 instead. One of the reasons as to why this old implementation of SSL/TLS is still being used, might be the fact that many legacy web browser such as IE6/XP only support SSL 3.0 and thus switching off support for SSL 3.0 would prevent browsers from working with the site \cite{owaspTLS_SSL}. The only IP addresses that are using SSL 3.0 also happen to be ones with self signed certificates as most certificates that are self signed are mainly used in the interal network and under less constrictions in terms on the policy that is enforced. Next we have TLSv1.0 which is the next version of TLS after SSLv3.0, similar advise is given with regards to the use of TLSv1.0 as it too is also a legacy protocol that suffers from problems such as the BEAST attack\cite{holz2015summarizing} which is an vulnerability in the implementation of the Cipher Block Chaining (CBC) in TLSv1.0. The most secure of the TLS versions found in the college has the greatest number of IP addresses (366) that are using it, to secure their communication. No version of TLS1.1 was ever discovered throughout the ZGrabs.

\subsection{Signature Algorithm}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/signatureAlgorithms}
\caption{a nice plot1}
\label{fig:signatureAlgorithms}
\end{figure}

One of the more surprising discoveries is the prevalence of MD5 hash function. In total we found 273 unique IP addresses that are using MD5withRSA to sign their certificates, of these 264 are self signed certificates. According to the Internet Engineering Task Force MD5 is no longer acceptable where collision resistance is required
such as digital signatures \cite{turner2011updated} this is due to the fact that MD5 hash functions allows the construction of different messages with the same MD5 hash resulting in a collision \cite{md5}. Along with MD5 being fundamental broken, SHA1 is also now insecure. Both of these signature algorithms in figure \ref{fig:signatureAlgorithms} used to sign certificates account for 63.7\% of 799 IP addresses in the college using unsecured hashing functions.\cite{ssllabs}. With the remaining 290 IP addresses possessing certificates signed with SHA256 which is the recommend signature algorithm to use\cite{ssllabs}.

\subsection{Cipher Suite}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/CipherSuitesInTCD}
\caption{a nice plot1}
\label{fig:cipherSuites}
\end{figure}

As we can see in figure \ref{fig:cipherSuites} $TLS_RSA_WITH_RC4_128_SHA$ is by far the most widely used cipher suite in the college, even though community at large advise against RC4 cipher suites as the bytes used to encrypt plaintext is not as random as one would hope resulting in attackers being able to exploit the bias in the keystream that encrypyt the plaintext, uncovering previous encrypted plaintext messages \cite{popov2015prohibiting}. The most least wide cipher suite found is  $TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA$ with only a single IP address offering up this in cipher suite to be used.

\subsection{Browser Trusted}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/BrowserTrusted}
\caption{a nice plot1}
\label{fig:browserTrusted}
\end{figure}
Of the certificates we managed to collect only 133 were browser trusted. Further more the cipher suites that browser trusted certificates use differs from that of the entire certificates collected with 45 IP address of using  $TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256$ cipher suites and only 17 using $TLS_RSA_WITH_RC4_128_SHA$. All the browser trusted certificates have been found to use SHA256withRSA as any certificate signed with MD5 or SHA1 is considered insecure \cite{ssllabs}.

\begin{table}[H]
\centering
\begin{tabular}{|| c c ||}
 \hline
  Issuer & Number of Certificates Issued  \\ [0.5ex]
 \hline\hline
TERENA SSL CA 2 & 10\\
COMODO RSA Domain Validation Secure Server CA &	2\\
Let's Encrypt Authority X3 & 7\\
DigiCert SHA2 High Assurance Server CA & 1\\
TERENA SSL CA 3 & 112 \\
GeoTrust DV SSL SHA256 CA & 1 \\[1ex]
 \hline
\end{tabular}
\caption{Browser Trusted Certificate Issuers }
\label{table:Browser_Trusted_Issuers}
\end{table}
There is a breakdown of the top issuers of broswer trusted certificates within TCD presented in table \ref{table:Browser_Trusted_Issuers} with Lets Encrypt who issue certificates with an expiry of 90 days \cite{LetsEncrypt} being the third most popular Issuer of browser trusted certificates in the college. The vast majority of public keys length used stays within the recommend value \cite{ssllabs} with 130 using public keys of length 2048, and remanining 3 using public keys of length 4096.


\subsection{Expired Certificates}
\begin{figure}[H]
\centering
\includegraphics[scale=.5]{pdf_images/CertificateExpired}
\caption{a nice plot1}
\label{fig:certExpired}
\end{figure}

From figure \ref{fig:certExpired} we can see that 18.8\% which is 150 IP addresses have certificates that are out of date with the most recent certificate expiring in 2018-03-16T15:49:14Z this certificate turned out to be a self signed cert, in terms of certificates that are signed by a CA the most expired on 2018-02-14T15:25:09Z. On the other scale the oldest expired certificates is a self signed cert with it's expire set to 1972-12-31T00:05:01Z, once in terms certs signed by CA the oldest being a cert that expired on 2014-08-01T23:59:59Z. One of the interesting findings in terms of the expired certificates is that there are 17 IP addresses with self signed certificates all of whom have certificate expired on 2007-01-01T00:00:00Z while also having their certificated beginning on the 2002-01-01T00:00:00Z, they also share the same public key length of 1024 bits as well as being signed with MD5withRSA along with the same cipher suite of $TLS_RSA_WITH_RC4_128_SHA$ all of these IP addresses turned out to be web servers of printer devices (print servers) specifically HP printers. What's interesting about this is that these devices could be an example of an employee being in charge of maintaining these devices and on leaving forgetting to mention that they exist. It also shows us that there is a good chance since these certificates haven't been updated that devices themselves could be using out of date software.

\subsection{Self Signed Certificates}

!!!!!!!!!!!!!!!!!!!!!!!!!!INSERT GRAPH OF SELF SIGNED CERTS!!!!!!!!!!!!!!!!!!!!!!!!!!!!

In total there were 555 IP addresses which had certificates that were self signed. The longest validity period found for a certificate was 27 years which was found on a single certificate with the shortest being valid for 12 weeks. Another finding was that of the 555 IP addresses that had self signed certificates 56 of them shared the same public key, which was the largest number of IP addresses that shared the same public key. they also shared the same issuer who had signed the certificates with MD5withRSA and had the same public key length of 1024 bits.


\begin{table}[H]
\centering
\begin{tabular}{||c c  ||}
 \hline
Public Key Length  & Number of Certificates\\ [0.5ex]
 \hline\hline
1024 & 380\\
768	& 2\\
4096 &	5\\
2048 &407\\
3072 & 1\\
512	& 4\\[1ex]

 \hline
\end{tabular}
\caption{Public key lengths of all IP addresses found in ZGrab scans on port 443}
\label{table:public_key_lengths}
\end{table}
Keeping with public keys, the self signed certificates found in the college contained the only 512 and 768 bit length certificates with regards to the rest of the certificates shown in table \ref{table:public_key_lengths}. The National Institute of Standards and Technology (NIST) recommends key sizes of at least 2056 bits, \cite{barker2015recommendation} this is due to successful attempts at breaking RSA by factoring for example 512 bit RSA keys were successful factored in 1999 \cite{kaliski2011rsa} at the moment large numbers are quite difficult to factor but the recommendation by nist is a way to stay ahead of the curve.

\subsection{Categorizing the Servers}
\begin{table}[H]
\centering
\begin{tabular}{||c c  ||}
 \hline
 Port 80 (only) & Port 443 (only)\\ [0.5ex]
 \hline\hline
Web Image Monitor& 16\\
Web Server & 21\\
Printer&15\\
DHCP Server&1\\
404&5\\
Other& 3\\
Hello World&13\\
Redirect&1\\
NAS Server&1\\
Conference Device&8\\
Projector&1\\[1ex]

 \hline
\end{tabular}
\caption{Categorizes of Irregular IP addresses}
\label{table:category}
\end{table}

Throughout this product there were 95 successful banner grabs of Irregular IP addresses along with their root page. 63 of these were only on port 80, 2 on port 443 and 30 having known open connections both ports. Of the 30 on both ports 27 were found on port 80 while 25 being found on port 443, 22 of these IP addresses were common between those IP addresses having known connections on both ports in each of the ZGrab scans on port 80 and port 443. Web Image monitors as seen in table \ref{table:category} were one of the most seen types of systems with all of them being exclusively on port 80 only, unfortunately this project hasn't been able to give an in depth information regarding these at the time of writing. Printers make up some of both the common and irregular IP addresses among the college campus with a variety of  models and devices such as HP, Lexmar and Cannon. Conferencing devices were also found with 7 of these being Polycom devices and the remaining being a Tanberg. There were 13 Hello World/test pages which could be either Student projects within the college or pages ensuring a server was functionally running. The actual Web Server pages  found consisted of Microsoft and Appache server pages. There were 3 Other that had no distinguishable way to categories them as seen in Table \ref{table:category} 
\chapter{Conclusion}

\cite{mendes2008assessing} page 316\\

This project investigates the state of Web Servers within Trinity College Dublin. 
Within this project I’ve introduced both ZMap a high speed port scanner and ZGrab an application banner grabber. I’ve also showed how these applications can be used to effectively answers questions regarding the current state of web servers within TCD such as to what was presented here in Chapter 1.\\

This project has allowed me to gain a deep understanding of the underlying technologies that aim to make the web more secure. Throughout this project I've been able to investigate interesting security topics such POODLE, BEAST and MD5 collisions.\\

The methodologies and design of the scans adopted in this project should allow other institutions and organisations who intend to replicate similar results presented here within their own private network as well as avoiding the challenges incurred in this journey.\\

With regards to IP addresses on port 443, the version of TLS used and the way it is configured is ultimatley up to the site owners and administrators to ensure that their servers are configured properly the approach taken in this project could be used to identify miss configured servers. For example over time cryptographic algorithms weaken, the methods used within this project could be implemented to locate servers offering weak cipher suites so that the correct configurations can be made. 

\chapter{Future Work}
While this report has demonstrated the use of ZMap and ZGrab on port 80 and port 443, there are many worthwhile areas of future work that could investigated.\\

Would be beneficial if we could find out the total number of host on port 80 and port 443 from system admins to determine whether or not the scans detecting all host on port 80 and port 443 in order to judge the successfulness of ZMap\\

Similarly to research discussed in Related Work \cite{durumeric2013zmap} \cite{durumeric2013analysis} it would be fascinating to see the adoption rate or change over time of IP addresses listening port 80 (HTTP) to ones listening on port 443 (HTTPS). This could carried out by running the scans implemented within this project over a longer time period to see these changes if at all any as well as investigating the possible reasons behind this, for example one reason could be the fact that Google now ranks sites implementing TLS/SSL as higher.\\


This report only detailed scans conducted on port 80 and port 443, in would be interesting to implement scanning on further ports in order to expand the view of activity that is currently on the college network.\\

Further expansion on the code produced within this project could be expanded in other to provide institutions/ organisations an easy to install application that would package the entire design, filtering and graphing of the scans conducted as well as producing daily summaries of the network in terms of number IP addresses listening and also notifying the user of any vulnerabilites or weak cipher suites being used such as servers using null cipher suite for example. This could also be extended to provide a ranking of the most secure web servers and more importantly the most vulnerable by using a weighting system similar to other researches \cite{mendes2008assessing} who have developed metrics for assessing web server which consist of a list of security recommendations that were taken from a set of interviews from security experts. The question of what database to use is a difficult question as discovered through the ZGrab scans that certain JSON objects produced could have a different number of fields, so if a certain field were not accounted for the schema of the database would not be compatible. Within the github of ZGrab \cite{zgrabGithub} there is the schema of all the fields that could be given out in the output of JSON file which could then be parsed with python code similar to what was produced in this project accounting for all the possible fields and then stored in a sqlLite database for example. One problem with this is that it those not take for the account of the possible updates to ZGrab which could involve the changing of the schema produced in the output of the JSON file along with this the creators are currently developing ZGrab 2.0 \cite{zgrab2Github} which will replace the current ZGrab, at the time of writing this report it's unsure at the moment what changes there will be between the two versions. While there doesn't seem to be clear cut solution to implement an application that is scalable, this would be an evaluable en devour.


\bibliographystyle{acm}
\bibliography{reference}


\appendix
\chapter{A first appendix}

blah blah \ldots


\chapter{A sample of the questionnaire form used}

blah blah


\end{document}

